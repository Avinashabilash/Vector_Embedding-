{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2676f37-0aa8-4abb-be08-b947a1b06d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1c9a58a-c7f6-47dd-a19a-4a22dc51ffd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abiav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6eae9414-806e-41fa-aeb3-1662092ccb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0df84bce-d8d4-41ad-81ac-3e1d1a721fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100  # matches GloVe file\n",
    "dataset_path = r\"C:\\Users\\abiav\\embedding vector\\male-female.txt\"\n",
    "glove_file = r\"C:\\Users\\abiav\\embedding vector\\glove.6B.100d.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b276ce0b-71ef-473d-875f-1e80ab6f800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "Total GloVe words: 400000\n"
     ]
    }
   ],
   "source": [
    "def load_glove(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0].lower()  # lowercase for consistency\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "embeddings = load_glove(glove_file)\n",
    "print(f\"Total GloVe words: {len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10b59d02-f40d-4d65-a927-c8afff418410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word pairs in dataset: 450\n"
     ]
    }
   ],
   "source": [
    "analogy_pairs = []\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            analogy_pairs.append((parts[0].lower(), parts[1].lower()))\n",
    "\n",
    "print(f\"Total word pairs in dataset: {len(analogy_pairs)}\")\n",
    "\n",
    "\n",
    "all_words = set([w for pair in analogy_pairs for w in pair if w in embeddings])\n",
    "word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "embedding_matrix = np.array([embeddings[word] for word in all_words])\n",
    "embedding_matrix = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03e26f35-a521-43a8-bf73-6340286ae763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_analogy(word_a, word_b, word_c, top_n=1):\n",
    "    \"\"\"Solve analogy: word_a is to word_b as word_c is to ?\"\"\"\n",
    "    for w in [word_a, word_b, word_c]:\n",
    "        if w not in word_to_idx:\n",
    "            return None \n",
    "            \n",
    "    vec_a = embedding_matrix[word_to_idx[word_a]]\n",
    "    vec_b = embedding_matrix[word_to_idx[word_b]]\n",
    "    vec_c = embedding_matrix[word_to_idx[word_c]]\n",
    "\n",
    "    target_vec = vec_b - vec_a + vec_c\n",
    "    target_vec /= np.linalg.norm(target_vec)\n",
    "\n",
    "    similarities = embedding_matrix @ target_vec\n",
    "    # exclude input words\n",
    "    for w in [word_a, word_b, word_c]:\n",
    "        similarities[word_to_idx[w]] = -np.inf\n",
    "\n",
    "    top_idx = np.argmax(similarities)\n",
    "    return idx_to_word[top_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "003fba67-333b-4174-8bc9-b4aa73f31b7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor-actress cosine similarity: 0.858\n",
      "batman-batwoman cosine similarity: 0.441\n",
      "boar-sow cosine similarity: 0.274\n",
      "boy-girl cosine similarity: 0.918\n",
      "brother-sister cosine similarity: 0.743\n",
      "buck-doe cosine similarity: 0.249\n",
      "bull-cow cosine similarity: 0.525\n",
      "businessman-businesswoman cosine similarity: 0.666\n",
      "chairman-chairwoman cosine similarity: 0.679\n",
      "dad-mom/mum cosine similarity: 0.063\n",
      "daddy-mommy/mother/mom cosine similarity: 0.078\n",
      "duke-duchess cosine similarity: 0.675\n",
      "emperor-empress cosine similarity: 0.735\n",
      "father-mother cosine similarity: 0.866\n",
      "fisherman-fisherwoman cosine similarity: 0.283\n",
      "fox-vixen cosine similarity: 0.126\n",
      "gentleman-lady/gentlewoman/madam cosine similarity: -0.035\n",
      "god-goddess cosine similarity: 0.619\n",
      "grandfather-grandmother cosine similarity: 0.803\n",
      "grandpa-grandma cosine similarity: 0.810\n",
      "grandson-granddaughter cosine similarity: 0.797\n",
      "groom-bride cosine similarity: 0.792\n",
      "headmaster-headmistress cosine similarity: 0.737\n",
      "heir-heiress cosine similarity: 0.653\n",
      "hero-heroine cosine similarity: 0.636\n",
      "hound-bitch cosine similarity: 0.492\n",
      "husband-wife cosine similarity: 0.922\n",
      "king-queen cosine similarity: 0.751\n",
      "lion-lioness cosine similarity: 0.454\n",
      "man-woman cosine similarity: 0.832\n",
      "manager-manageress cosine similarity: -0.112\n",
      "mister-miss/missis/missus/mis'ess/mrs/ms/madam cosine similarity: -0.055\n",
      "murderer-murderess cosine similarity: 0.402\n",
      "nephew-niece cosine similarity: 0.812\n",
      "poet-poetess cosine similarity: 0.420\n",
      "policeman-policewoman cosine similarity: 0.546\n",
      "prince-princess cosine similarity: 0.707\n",
      "ram-ewe cosine similarity: 0.204\n",
      "rooster-hen cosine similarity: 0.501\n",
      "sculptor-sculptress cosine similarity: 0.307\n",
      "sir-madam cosine similarity: 0.123\n",
      "son-daughter cosine similarity: 0.865\n",
      "stallion-mare cosine similarity: 0.525\n",
      "stepfather-stepmother cosine similarity: 0.733\n",
      "superman-superwoman cosine similarity: 0.355\n",
      "tiger-tigress cosine similarity: 0.262\n",
      "uncle-aunt cosine similarity: 0.759\n",
      "valet-maid/maidservant/housemaid/chambermaid/handmaid/handmaiden/parlormaid/parlourmaid cosine similarity: 0.124\n",
      "waiter-waitress cosine similarity: 0.777\n",
      "webmaster-webmistress cosine similarity: -0.224\n",
      "andorra-catalan cosine similarity: 0.252\n",
      "argentina-spanish cosine similarity: 0.529\n",
      "australia-english cosine similarity: 0.420\n",
      "austria-german cosine similarity: 0.564\n",
      "bahamas-english cosine similarity: -0.011\n",
      "bangladesh-bengali/bangla cosine similarity: -0.096\n",
      "barbados-english cosine similarity: 0.168\n",
      "belize-english cosine similarity: 0.083\n",
      "bolivia-spanish cosine similarity: 0.358\n",
      "brazil-portuguese cosine similarity: 0.564\n",
      "cambodia-khmer cosine similarity: 0.603\n",
      "canada-english/french cosine similarity: 0.125\n",
      "chile-spanish cosine similarity: 0.448\n",
      "colombia-spanish cosine similarity: 0.480\n",
      "cuba-spanish cosine similarity: 0.412\n",
      "cyprus-greek/turkish cosine similarity: -0.039\n",
      "denmark-danish/faroese/greenlandic/german cosine similarity: 0.108\n",
      "ecuador-spanish/quechua cosine similarity: -0.213\n",
      "egypt-arabic cosine similarity: 0.393\n",
      "ethiopia-amharic cosine similarity: 0.277\n",
      "fiji-english cosine similarity: 0.214\n",
      "guadeloupe-french cosine similarity: 0.355\n",
      "guam-english/chamorro cosine similarity: 0.026\n",
      "guatemala-spanish cosine similarity: 0.373\n",
      "guyana-english cosine similarity: 0.141\n",
      "haiti-creole/french cosine similarity: -0.138\n",
      "iran-persian cosine similarity: 0.452\n",
      "iraq-arabic/kurdish cosine similarity: -0.051\n",
      "ireland-english/irish/gaelic cosine similarity: -0.007\n",
      "israel-hebrew/arabic cosine similarity: -0.031\n",
      "jamaica-english/creole cosine similarity: -0.024\n",
      "jordan-arabic cosine similarity: 0.271\n",
      "kazakhstan-kazak/qazaq/russian cosine similarity: 0.131\n",
      "kosovo-albanian/serbian cosine similarity: 0.042\n",
      "kuwait-arabic cosine similarity: 0.309\n",
      "mexico-spanish/nahuatl cosine similarity: -0.083\n",
      "moldova-moldovan/romanian cosine similarity: -0.116\n",
      "morocco-berber/arabic cosine similarity: -0.102\n",
      "mozambique-portuguese cosine similarity: 0.445\n",
      "netherlands-dutch/frisian cosine similarity: -0.057\n",
      "nicaragua-spanish cosine similarity: 0.310\n",
      "norway-norwegian/bokm√•l/bokmal/nynorsk cosine similarity: 0.046\n",
      "palestine-arabic cosine similarity: 0.369\n",
      "peru-spanish cosine similarity: 0.460\n",
      "philippines-tagalog/filipino cosine similarity: -0.136\n",
      "switzerland-german/french/italian cosine similarity: -0.148\n",
      "syria-arabic cosine similarity: 0.357\n",
      "taiwan-chinese cosine similarity: 0.750\n",
      "usa-english cosine similarity: 0.272\n",
      "venezuela-spanish cosine similarity: 0.447\n",
      "ant-black/brown/red cosine similarity: 0.120\n",
      "apple-red/orange/yellow/golden cosine similarity: -0.133\n",
      "blackboard-black/green cosine similarity: -0.012\n",
      "blood-red cosine similarity: 0.473\n",
      "blueberry-blue/black cosine similarity: 0.075\n",
      "broccoli-green cosine similarity: 0.259\n",
      "bruise-blue/purple cosine similarity: -0.174\n",
      "cabbage-green cosine similarity: 0.387\n",
      "carrot-orange/red/yellow cosine similarity: 0.085\n",
      "cauliflower-white/green/yellow/yellowish cosine similarity: -0.128\n",
      "celery-green/white/brown cosine similarity: -0.051\n",
      "cherry-red/yellow/black cosine similarity: 0.018\n",
      "chocolate-white/brown/black cosine similarity: 0.013\n",
      "cloud-white/gray/grey cosine similarity: -0.045\n",
      "coal-black cosine similarity: 0.240\n",
      "coffee-black/brown cosine similarity: -0.017\n",
      "cranberry-red/purple/pink cosine similarity: 0.037\n",
      "cream-white cosine similarity: 0.384\n",
      "crow-black cosine similarity: 0.439\n",
      "cucumber-green cosine similarity: 0.320\n",
      "emerald-green cosine similarity: 0.431\n",
      "fridge-white/silver/black cosine similarity: -0.129\n",
      "frog-green/brown/grey/gray cosine similarity: 0.003\n",
      "grapes-black/red/green/purple cosine similarity: -0.013\n",
      "grass-green cosine similarity: 0.615\n",
      "leaves-green/red/yellow cosine similarity: -0.075\n",
      "milk-white cosine similarity: 0.302\n",
      "paper-white/color cosine similarity: 0.005\n",
      "parsley-green cosine similarity: 0.277\n",
      "peony-red/white/pink/purple cosine similarity: -0.062\n",
      "pepper-black/red/green/yellow/orange cosine similarity: 0.121\n",
      "potato-brown cosine similarity: 0.356\n",
      "radish-red/pink/white/green/black cosine similarity: -0.166\n",
      "raven-black cosine similarity: 0.402\n",
      "rose-red/yellow/pink/white/blue cosine similarity: 0.134\n",
      "ruby-red cosine similarity: 0.354\n",
      "salt-white cosine similarity: 0.388\n",
      "sapphire-blue cosine similarity: 0.366\n",
      "sea-blue/green/gray/grey cosine similarity: -0.176\n",
      "sky-blue/gray/grey cosine similarity: 0.071\n",
      "snow-white cosine similarity: 0.426\n",
      "soil-black/brown/dark cosine similarity: 0.028\n",
      "spinach-green cosine similarity: 0.277\n",
      "sugar-white/brown cosine similarity: 0.072\n",
      "sun-yellow/gold cosine similarity: -0.087\n",
      "swan-white/black/gray/grey cosine similarity: -0.052\n",
      "tea-black/green/white/red/brown/yellow cosine similarity: -0.051\n",
      "tomato-red cosine similarity: 0.390\n",
      "toothpaste-white cosine similarity: 0.124\n",
      "yoghurt-white/pink cosine similarity: -0.148\n",
      "ant-anthill/insectarium/terrarium/formicarium cosine similarity: 0.159\n",
      "ape-grove/tree/cage cosine similarity: -0.052\n",
      "baboon-grove/tree/cage cosine similarity: 0.041\n",
      "bat-cave/cage cosine similarity: -0.005\n",
      "bear-den/cage cosine similarity: -0.052\n",
      "beaver-dam/river/pen cosine similarity: -0.029\n",
      "bee-hive cosine similarity: 0.459\n",
      "cattle-barn/coral cosine similarity: -0.060\n",
      "chimpanzee-grove/tree/cage cosine similarity: -0.062\n",
      "chinchilla-nest/cage cosine similarity: -0.103\n",
      "cockroach-nest cosine similarity: 0.269\n",
      "cricket-nest cosine similarity: 0.064\n",
      "crocodile-river/lake/pool cosine similarity: 0.047\n",
      "crow-nest/cage cosine similarity: 0.070\n",
      "dog-doghouse/home/den/kennel cosine similarity: 0.176\n",
      "dolphin-sea/sanctuary cosine similarity: -0.006\n",
      "duck-pond/nest cosine similarity: -0.042\n",
      "fish-sea/lake/river/acquarium/farm/sanctuary cosine similarity: -0.055\n",
      "fly-nest cosine similarity: 0.404\n",
      "fox-den/cage cosine similarity: 0.251\n",
      "goldfish-pond/bowl/aquarium/sanctuary cosine similarity: -0.058\n",
      "gorilla-grove/tree/cage cosine similarity: 0.078\n",
      "hamster-nest/cage cosine similarity: 0.086\n",
      "hedgehog-nest/hedge/pen cosine similarity: -0.094\n",
      "herring-sea/sanctuary cosine similarity: -0.086\n",
      "hippopotamus-river/lake/pen cosine similarity: -0.058\n",
      "hornet-nest cosine similarity: 0.306\n",
      "horse-stable/range/paddock/corral cosine similarity: -0.013\n",
      "insect-nest/cage/box cosine similarity: 0.092\n",
      "lion-den/cage/savannah cosine similarity: -0.013\n",
      "locust-nest cosine similarity: 0.218\n",
      "mallard-nest/pond cosine similarity: 0.101\n",
      "mole-hole/nest cosine similarity: -0.011\n",
      "monkey-tree/grove/cage cosine similarity: 0.008\n",
      "mouse-nest/cage cosine similarity: 0.096\n",
      "pig-sty/pigsty/pen/pigpen cosine similarity: -0.039\n",
      "rabbit-burrow/warren/hutch/cage cosine similarity: 0.023\n",
      "rat-nest/cage cosine similarity: -0.048\n",
      "raven-nest/cage cosine similarity: 0.062\n",
      "scorpion-nest/aquarium/terrarium cosine similarity: 0.088\n",
      "seal-den/aquarium/sea cosine similarity: -0.081\n",
      "snake-nest/pit/acquarium cosine similarity: 0.069\n",
      "spider-web/acquarium/terrarium cosine similarity: 0.143\n",
      "termite-hill/terrarium cosine similarity: 0.118\n",
      "tiger-den/cage cosine similarity: -0.101\n",
      "trout-river/lake/sanctuary/aquarium/pond/tank cosine similarity: -0.015\n",
      "wasp-nest cosine similarity: 0.423\n",
      "whale-sea/sanctuary cosine similarity: -0.028\n",
      "wolf-den/cage cosine similarity: -0.056\n",
      "woodchuck-hole cosine similarity: 0.112\n",
      "alpaca-bray cosine similarity: 0.033\n",
      "bear-growl cosine similarity: 0.148\n",
      "bee-buzz/hum cosine similarity: -0.055\n",
      "beetle-drone cosine similarity: 0.128\n",
      "cat-meow/meu/purr/caterwaul cosine similarity: 0.018\n",
      "cattle-moo/bellow/low cosine similarity: 0.028\n",
      "chicken-cluck/crow/cock-a-doodle-doo cosine similarity: 0.202\n",
      "chimpanzee-scream cosine similarity: 0.054\n",
      "cicada-buzz cosine similarity: -0.044\n",
      "coyote-howl cosine similarity: 0.266\n",
      "cricket-chirp cosine similarity: -0.051\n",
      "crow-caw cosine similarity: 0.018\n",
      "deer-bellow cosine similarity: 0.099\n",
      "dog-bark/growl/howl/yelp/whine/arf/bow_wow/woof cosine similarity: -0.056\n",
      "donkey-bray/hee-haw cosine similarity: -0.106\n",
      "duck-quack cosine similarity: 0.334\n",
      "elephant-trumpet cosine similarity: 0.276\n",
      "elk-bellow cosine similarity: 0.108\n",
      "ferret-dook cosine similarity: -0.030\n",
      "fly-buzz cosine similarity: 0.290\n",
      "fox-howl/yelp cosine similarity: -0.050\n",
      "frog-ribbit/croak cosine similarity: 0.104\n",
      "goat-bleat cosine similarity: 0.171\n",
      "gorilla-grunt/scream cosine similarity: -0.002\n",
      "hornet-buzz cosine similarity: 0.208\n",
      "horse-neigh/snort/whinny cosine similarity: 0.033\n",
      "hound-bark/howl/bay cosine similarity: -0.112\n",
      "hyena-laugh cosine similarity: 0.139\n",
      "leopard-growl cosine similarity: 0.160\n",
      "lion-roar/growl cosine similarity: -0.159\n",
      "magpie-chatter cosine similarity: -0.056\n",
      "mallard-quack cosine similarity: 0.228\n",
      "monkey-chatter/gibber/howl/scream cosine similarity: 0.230\n",
      "moose-bellow cosine similarity: 0.039\n",
      "mouse-squeak cosine similarity: 0.245\n",
      "mule-bray/hee-haw cosine similarity: -0.098\n",
      "pig-oink/grunt/gruff/squeal cosine similarity: 0.132\n",
      "pigeon-coo cosine similarity: 0.047\n",
      "rat-squeak cosine similarity: 0.194\n",
      "raven-caw cosine similarity: 0.098\n",
      "seal-bark cosine similarity: 0.292\n",
      "sheep-baa/bleat cosine similarity: 0.082\n",
      "snake-hiss cosine similarity: 0.223\n",
      "songbird-chirrup/chirp/tweet/sing/warble/twitter cosine similarity: -0.104\n",
      "tiger-growl/roar cosine similarity: 0.123\n",
      "toad-ribbit/croak cosine similarity: 0.014\n",
      "turkey-gobble cosine similarity: -0.050\n",
      "wasp-buzz cosine similarity: 0.087\n",
      "whale-sing cosine similarity: 0.161\n",
      "wolf-howl cosine similarity: 0.388\n",
      "ape-baby/infant cosine similarity: -0.039\n",
      "badger-kit/cob cosine similarity: -0.067\n",
      "bear-cub cosine similarity: 0.440\n",
      "beaver-kit/kitten cosine similarity: 0.118\n",
      "bee-larva cosine similarity: 0.134\n",
      "beetle-larva cosine similarity: 0.471\n",
      "buffalo-calf cosine similarity: 0.296\n",
      "butterfly-larva/pupa/caterpillar/chrysalis cosine similarity: 0.054\n",
      "camel-calf/colt cosine similarity: 0.040\n",
      "cat-kitten cosine similarity: 0.558\n",
      "cattle-calf/heifer cosine similarity: 0.128\n",
      "chimpanzee-baby/infant cosine similarity: 0.001\n",
      "cicada-nymph cosine similarity: 0.337\n",
      "cockroach-nymph cosine similarity: 0.448\n",
      "cricket-larva cosine similarity: -0.122\n",
      "deer-fawn cosine similarity: 0.304\n",
      "dog-puppy/pup/whelp cosine similarity: 0.141\n",
      "duck-duckling cosine similarity: 0.236\n",
      "elephant-calf cosine similarity: 0.455\n",
      "ferret-kit cosine similarity: 0.067\n",
      "fish-fingerling/spawn/egg/larva/fry/minnmow cosine similarity: 0.064\n",
      "fly-grub/maggot cosine similarity: 0.019\n",
      "fox-cub/pup/puppy/whelp cosine similarity: 0.110\n",
      "goat-kid cosine similarity: 0.399\n",
      "goldfish-fingerling/fry cosine similarity: -0.232\n",
      "gorilla-infant cosine similarity: 0.300\n",
      "herring-fingerling/fry cosine similarity: -0.080\n",
      "horse-foal/colt/filly cosine similarity: 0.008\n",
      "insect-larva cosine similarity: 0.479\n",
      "lion-cub cosine similarity: 0.436\n",
      "mink-kit/cub cosine similarity: 0.084\n",
      "monkey-infant cosine similarity: 0.303\n",
      "muskrat-kit cosine similarity: -0.169\n",
      "ox-calf/stot cosine similarity: -0.171\n",
      "panda-cub cosine similarity: 0.720\n",
      "pig-piglet/shoat/farrow cosine similarity: 0.092\n",
      "rabbit-bunny cosine similarity: 0.740\n",
      "raccoon-kit/cub cosine similarity: 0.052\n",
      "salmon-smolt cosine similarity: 0.289\n",
      "seal-pup cosine similarity: 0.353\n",
      "shark-cub/pup cosine similarity: -0.020\n",
      "sheep-lamb/lambkin/cosset cosine similarity: -0.056\n",
      "skunk-kit/kitten cosine similarity: 0.077\n",
      "snake-hatchling/nestling cosine similarity: 0.290\n",
      "tiger-cub cosine similarity: 0.418\n",
      "trout-fingerling cosine similarity: 0.302\n",
      "weasel-kit cosine similarity: 0.073\n",
      "whale-calf cosine similarity: 0.409\n",
      "wolf-cub/pup/puppy/whelp cosine similarity: 0.045\n",
      "woodchuck-kit/cob cosine similarity: -0.019\n",
      "andersen-writer/poet/author cosine similarity: -0.052\n",
      "aristotle-philosopher cosine similarity: 0.530\n",
      "balzac-novelist/writer cosine similarity: 0.038\n",
      "beethoven-composer cosine similarity: 0.556\n",
      "caesar-emperor/commander/leader cosine similarity: -0.066\n",
      "columbus-explorer cosine similarity: 0.197\n",
      "confucius-philosopher cosine similarity: 0.428\n",
      "dante-poet cosine similarity: 0.401\n",
      "darwin-naturalist/biologist/geologist cosine similarity: 0.005\n",
      "depp-actor/producer/musician cosine similarity: 0.006\n",
      "descartes-mathematician/philosopher cosine similarity: -0.031\n",
      "dickens-novelist/writer/critic/author cosine similarity: 0.039\n",
      "edison-inventor/businessman cosine similarity: -0.055\n",
      "einstein-physicist/scientist cosine similarity: -0.186\n",
      "euler-mathematician/physicist/astronomer/logician/engineer cosine similarity: 0.153\n",
      "goethe-poet/playwright/novelist/writer/author cosine similarity: -0.156\n",
      "hawking-physicist/scientist cosine similarity: 0.057\n",
      "haydn-composer cosine similarity: 0.501\n",
      "hegel-philosopher cosine similarity: 0.486\n",
      "hitler-dictator/politician/nazi cosine similarity: 0.017\n",
      "hume-philosopher/politician cosine similarity: 0.103\n",
      "jolie-actress/filmmaker/director/humanitarian/activist cosine similarity: -0.061\n",
      "kant-philosopher cosine similarity: 0.465\n",
      "kepler-mathematician/physicist/astronomer/astrologer cosine similarity: -0.207\n",
      "lincoln-president cosine similarity: 0.438\n",
      "locke-philosopher cosine similarity: 0.380\n",
      "marx-philosopher/communist cosine similarity: -0.084\n",
      "maxwell-physicist/scientist cosine similarity: -0.124\n",
      "mencius-philosopher cosine similarity: 0.458\n",
      "michelangelo-sculptor/painter/architect/artist/poet/engineer cosine similarity: -0.015\n",
      "moses-prophet/leader cosine similarity: -0.014\n",
      "mozart-composer cosine similarity: 0.606\n",
      "napoleon-emperor/leader/politician/commander cosine similarity: -0.212\n",
      "newton-scientist/mathematician/psysicist/philosopher cosine similarity: 0.011\n",
      "pacino-actor/director/filmmaker cosine similarity: -0.055\n",
      "pascal-mathematician/philosopher cosine similarity: 0.235\n",
      "picasso-painter/artist/sculptor/designer cosine similarity: -0.110\n",
      "plato-philosopher cosine similarity: 0.525\n",
      "raphael-painter/artist/architect cosine similarity: -0.067\n",
      "rembrandt-painter/etcher/artist cosine similarity: -0.065\n",
      "rousseau-writer/author/philosopher cosine similarity: 0.131\n",
      "schwarzenegger-actor/politician/governor cosine similarity: -0.013\n",
      "shakespeare-playwright/poet cosine similarity: -0.053\n",
      "spinoza-philosopher cosine similarity: 0.482\n",
      "stalin-dictator/politician/leader/statesman cosine similarity: -0.056\n",
      "strauss-composer cosine similarity: 0.346\n",
      "tolstoi-novelist/writer/philosopher cosine similarity: -0.004\n",
      "truman-president cosine similarity: 0.427\n",
      "wagner-composer cosine similarity: 0.519\n",
      "wittgenstein-philosopher cosine similarity: 0.536\n",
      "aristotle-greek cosine similarity: 0.374\n",
      "balzac-french cosine similarity: 0.209\n",
      "beethoven-german cosine similarity: 0.248\n",
      "caesar-roman cosine similarity: 0.418\n",
      "confucius-chinese cosine similarity: 0.380\n",
      "copernicus-polish cosine similarity: 0.108\n",
      "darwin-english/british cosine similarity: -0.052\n",
      "depp-american cosine similarity: 0.104\n",
      "descartes-french cosine similarity: 0.151\n",
      "dickens-english/british cosine similarity: 0.106\n",
      "dostoyevsky-russian cosine similarity: 0.217\n",
      "edison-american cosine similarity: 0.178\n",
      "einstein-jewish/german/american cosine similarity: -0.042\n",
      "euclid-greek cosine similarity: 0.061\n",
      "fermi-italian cosine similarity: 0.065\n",
      "galilei-italian cosine similarity: 0.096\n",
      "gorbachev-soviet/russian cosine similarity: 0.052\n",
      "hawking-english/british cosine similarity: 0.141\n",
      "hegel-german cosine similarity: 0.191\n",
      "hitler-german/austrian cosine similarity: 0.054\n",
      "homer-greek cosine similarity: 0.114\n",
      "hume-scottish/british cosine similarity: 0.037\n",
      "jolie-american cosine similarity: 0.014\n",
      "kant-german cosine similarity: 0.082\n",
      "kepler-german cosine similarity: 0.022\n",
      "lavoisier-french cosine similarity: 0.057\n",
      "leibniz-german cosine similarity: 0.110\n",
      "lenin-soviet/russian cosine similarity: 0.040\n",
      "lennon-english/british cosine similarity: 0.112\n",
      "lincoln-american cosine similarity: 0.410\n",
      "locke-english/british cosine similarity: 0.173\n",
      "machiavelli-italian cosine similarity: 0.096\n",
      "marx-german cosine similarity: 0.255\n",
      "maxwell-scottish/british cosine similarity: 0.058\n",
      "mencius-chinese cosine similarity: 0.114\n",
      "michelangelo-italian cosine similarity: 0.234\n",
      "mozart-german/austrian cosine similarity: -0.058\n",
      "napoleon-french/corsican/italian cosine similarity: 0.136\n",
      "newton-english/british cosine similarity: 0.088\n",
      "pascal-french cosine similarity: 0.248\n",
      "plato-greek cosine similarity: 0.334\n",
      "raphael-italian cosine similarity: 0.201\n",
      "rousseau-french cosine similarity: 0.238\n",
      "spinoza-dutch cosine similarity: -0.065\n",
      "stalin-soviet/georgian cosine similarity: -0.138\n",
      "strauss-austrian cosine similarity: 0.185\n",
      "tchaikovsky-russian cosine similarity: 0.311\n",
      "tolstoi-russian cosine similarity: -0.078\n",
      "truman-american cosine similarity: 0.299\n",
      "wagner-german cosine similarity: 0.313\n",
      "aberdeen-aberdeenshire cosine similarity: 0.512\n",
      "bath-somerset cosine similarity: 0.471\n",
      "belfast-antrim cosine similarity: 0.577\n",
      "birmingham-midlands cosine similarity: 0.627\n",
      "bradford-yorkshire cosine similarity: 0.673\n",
      "brighton-sussex cosine similarity: 0.717\n",
      "cambridge-cambridgeshire cosine similarity: 0.419\n",
      "canterbury-kent cosine similarity: 0.481\n",
      "cardiff-glamorgan cosine similarity: 0.643\n",
      "carlisle-cumbria cosine similarity: 0.579\n",
      "chester-cheshire cosine similarity: 0.705\n",
      "chichester-sussex cosine similarity: 0.571\n",
      "coventry-midlands cosine similarity: 0.595\n",
      "crawley-sussex cosine similarity: 0.620\n",
      "derby-derbyshire cosine similarity: 0.411\n",
      "dundee-lowlands cosine similarity: 0.190\n",
      "edinburgh-lowlands cosine similarity: 0.136\n",
      "ely-cambridgeshire cosine similarity: 0.451\n",
      "exeter-devon cosine similarity: 0.642\n",
      "glasgow-lowlands cosine similarity: 0.150\n",
      "gloucester-gloucestershire cosine similarity: 0.617\n",
      "hereford-herefordshire cosine similarity: 0.658\n",
      "hull-yorkshire cosine similarity: 0.412\n",
      "inverness-highlands cosine similarity: 0.408\n",
      "lancaster-lancashire cosine similarity: 0.565\n",
      "leeds-yorkshire cosine similarity: 0.685\n",
      "leicester-midlands cosine similarity: 0.539\n",
      "lincoln-lincolnshire cosine similarity: 0.345\n",
      "liverpool-lancashire cosine similarity: 0.532\n",
      "newcastle-northumberland cosine similarity: 0.459\n",
      "newport-gwent cosine similarity: 0.609\n",
      "norwich-norfolk cosine similarity: 0.580\n",
      "nottingham-nottinghamshire cosine similarity: 0.619\n",
      "oxford-oxfordshire cosine similarity: 0.492\n",
      "plymouth-devon cosine similarity: 0.565\n",
      "portsmouth-hampshire cosine similarity: 0.428\n",
      "preston-lancashire cosine similarity: 0.610\n",
      "reading-berkshire cosine similarity: 0.228\n",
      "salford-manchester cosine similarity: 0.642\n",
      "salisbury-wiltshire cosine similarity: 0.621\n",
      "sheffield-yorkshire cosine similarity: 0.588\n",
      "southampton-hampshire cosine similarity: 0.462\n",
      "stirling-stirlingshire cosine similarity: 0.320\n",
      "swansea-glamorgan cosine similarity: 0.704\n",
      "wakefield-yorkshire cosine similarity: 0.607\n",
      "wells-somerset cosine similarity: 0.286\n",
      "winchester-hampshire cosine similarity: 0.475\n",
      "wolverhampton-midlands cosine similarity: 0.659\n",
      "worcester-worcestershire cosine similarity: 0.525\n",
      "york-yorkshire cosine similarity: 0.279\n"
     ]
    }
   ],
   "source": [
    "for w1, w2 in analogy_pairs:\n",
    "    if w1 not in embeddings:\n",
    "        embeddings[w1] = np.random.uniform(-1, 1, embedding_dim)\n",
    "    if w2 not in embeddings:\n",
    "        embeddings[w2] = np.random.uniform(-1, 1, embedding_dim)\n",
    "\n",
    "    vec1 = embeddings[w1]\n",
    "    vec2 = embeddings[w2]\n",
    "    cos_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    print(f\"{w1}-{w2} cosine similarity: {cos_sim:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7b5124a-275d-4f52-bb86-4c3e903411ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "dataset_path = r\"C:\\Users\\abiav\\embedding vector\\E10 [male - female].txt\"\n",
    "glove_file = r\"C:\\Users\\abiav\\embedding vector\\glove.6B.100d.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db095f8a-ea44-4c40-9758-0cc1764ddd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0].lower()\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "embeddings = load_glove(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1d4e93f-7e9b-4730-9afa-f61928f222d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_pairs = []\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            analogy_pairs.append((parts[0].lower(), parts[1].lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a333f18-1568-4290-a2f4-dd58ad19c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_words = [w2 for _, w2 in analogy_pairs if w2 in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "add40f70-4b92-4949-b933-453105f49623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    parts = word.split('/')  \n",
    "    vectors = [embeddings[p] for p in parts if p in embeddings]\n",
    "    if vectors:\n",
    "        vec = np.mean(vectors, axis=0)\n",
    "        return vec / np.linalg.norm(vec)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be19c0e7-27aa-4386-bac7-5493ab6e0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for w1, w2 in analogy_pairs:\n",
    "    vec1 = get_vector(w1)\n",
    "    vec2 = get_vector(w2)\n",
    "    if vec1 is None or vec2 is None:\n",
    "        continue\n",
    "\n",
    "    # Compute cosine similarity with all female words\n",
    "    sims = []\n",
    "    for f in female_words:\n",
    "        f_vec = get_vector(f)\n",
    "        if f_vec is not None:\n",
    "            sims.append((f, np.dot(vec1, f_vec)))\n",
    "    if not sims:\n",
    "        continue\n",
    "\n",
    "    # Pick female word with max similarity\n",
    "    pred_word, _ = max(sims, key=lambda x: x[1])\n",
    "\n",
    "    if pred_word == w2:\n",
    "        correct += 1\n",
    "    total += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3af75325-d75a-4676-9894-16975c986fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male-Female Analogy Accuracy: 0.59\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f\"Male-Female Analogy Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
